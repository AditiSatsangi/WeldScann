{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WLswwIaIZ-WY"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "WLswwIaIZ-WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "metadata": {
        "id": "Eyap5-ZTaEA7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XX6tkYm5USN0",
        "outputId": "6d90a781-b263-4e87-fdd0-928d9f7a1c5f",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.43)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (11.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.3.57-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading ultralytics-8.3.57-py3-none-any.whl (905 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.3.57 ultralytics-thop-2.0.13\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unified Model Architecture"
      ],
      "metadata": {
        "id": "EriOdon7mh-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Image\n",
        "   → Pretrained YOLO (Feature Extraction + Bounding Box Predictions)\n",
        "      → ROI Pooling Layer (Extract Features for Each Bounding Box)\n",
        "         → Classifier Head (Fine-tuned VGG16 or Custom CNN for Defect Classification)\n",
        "            → Combined Output (Bounding Boxes + Defect Labels)\n"
      ],
      "metadata": {
        "id": "FwxZb0BImcUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfetUNcQVFYQ",
        "outputId": "c77d149c-2d57-45a4-ca44-adfda508f599",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics tensorflow keras\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t9cx3aYnjBh",
        "outputId": "bf4a5601-3a59-4ac0-c8dd-676aeca8d5c5",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.88-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Downloading ultralytics-8.3.88-py3-none-any.whl (932 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.9/932.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.88 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loding dataset"
      ],
      "metadata": {
        "id": "Ok8RUY6yo0ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the uploaded file\n",
        "zip_file_path = '/content/train.zip'\n",
        "\n",
        "# Path to extract the contents\n",
        "extract_path = '/content/train'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJd7FWcZpYPb",
        "outputId": "b8b84b6d-9542-43c3-b59c-d42a79a781b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/train\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the uploaded file\n",
        "zip_file_path = '/content/test.zip'\n",
        "\n",
        "# Path to extract the contents\n",
        "extract_path = '/content/test'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcN0jmFQpZdm",
        "outputId": "83fb1f01-8312-4e1d-bae1-5b7c59f2ac50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/test\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Dataset loader with robustness\n",
        "def load_dataset(image_dir, label_dir, target_size=(640, 640)):\n",
        "    images = []\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "    missing_labels = []\n",
        "    corrupt_labels = []\n",
        "\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        try:\n",
        "            # Load image\n",
        "            img_path = os.path.join(image_dir, img_file)\n",
        "            image = tf.io.read_file(img_path)\n",
        "            image = tf.image.decode_jpeg(image, channels=3)\n",
        "            image = tf.image.resize(image, target_size) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "            # Load corresponding label\n",
        "            label_file = os.path.join(label_dir, img_file.replace('.jpg', '.txt'))\n",
        "            if not os.path.exists(label_file):\n",
        "                missing_labels.append(img_file)\n",
        "                continue  # Skip if label file is missing\n",
        "\n",
        "            with open(label_file, 'r') as f:\n",
        "                label_data = f.readlines()\n",
        "\n",
        "            bbox = []\n",
        "            class_ids = []\n",
        "            for line in label_data:\n",
        "                try:\n",
        "                    # Parse YOLO label format\n",
        "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    class_ids.append(int(class_id))\n",
        "\n",
        "                    # Convert YOLO format to [x_min, y_min, x_max, y_max]\n",
        "                    x_min = (x_center - width / 2) * target_size[0]\n",
        "                    y_min = (y_center - height / 2) * target_size[1]\n",
        "                    x_max = (x_center + width / 2) * target_size[0]\n",
        "                    y_max = (y_center + height / 2) * target_size[1]\n",
        "                    bbox.append([x_min, y_min, x_max, y_max])\n",
        "                except ValueError:\n",
        "                    corrupt_labels.append(label_file)\n",
        "                    break  # Skip this image if label parsing fails\n",
        "\n",
        "            if bbox:  # Add only if bounding boxes are valid\n",
        "                images.append(image)\n",
        "                bboxes.append(bbox)\n",
        "                labels.append(class_ids)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {img_file}: {e}\")\n",
        "\n",
        "    if missing_labels:\n",
        "        print(f\"Missing labels for {len(missing_labels)} images.\")\n",
        "    if corrupt_labels:\n",
        "        print(f\"Corrupt labels in {len(corrupt_labels)} files.\")\n",
        "\n",
        "    return images, bboxes, labels\n",
        "\n",
        "# Paths to train/test folders\n",
        "train_image_dir = '/content/drive/MyDrive/Building_Defects/Dataset/train/images'\n",
        "train_label_dir = '/content/drive/MyDrive/Building_Defects/Dataset/train/labels'\n",
        "test_image_dir = '/content/drive/MyDrive/Building_Defects/Dataset/test/images'\n",
        "test_label_dir = '/content/drive/MyDrive/Building_Defects/Dataset/test/labels'\n",
        "\n",
        "# Load train and test datasets\n",
        "train_images, train_bboxes, train_labels = load_dataset(train_image_dir, train_label_dir)\n",
        "test_images, test_bboxes, test_labels = load_dataset(test_image_dir, test_label_dir)\n",
        "\n",
        "print(f\"Loaded {len(train_images)} training images and {len(test_images)} test images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqi4_O06r2ef",
        "outputId": "0907dfca-1dcc-461b-806f-5c7bb6c5112e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing labels for 143 images.\n",
            "Missing labels for 143 images.\n",
            "Loaded 573 training images and 37 test images.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths for train and test datasets\n",
        "train_images_path = \"/content/drive/MyDrive/Building_Defects/train/images\"\n",
        "train_labels_path = \"/content/drive/MyDrive/Building_Defects/train/labels\"\n",
        "test_images_path = \"/content/drive/MyDrive/Building_Defect/test/images\"\n",
        "test_labels_path = \"/content/drive/MyDrive/Building_Defects/test/labels\"\n",
        "\n",
        "combined_images_path = \"/content/drive/MyDrive/Building_Defects/Dataset/combined/images\"\n",
        "combined_labels_path = \"/content/drive/MyDrive/Building_Defects/Dataset/combined/labels\"\n",
        "\"\"\"\n",
        "# Paths for train and test datasets\n",
        "train_images_path = \"/content/Dataset/train/images\"\n",
        "train_labels_path = \"/content/Dataset/train/labels\"\n",
        "test_images_path = \"/content/Dataset/test/images\"\n",
        "test_labels_path = \"/content/Dataset/test/labels\"\n",
        "\n",
        "combined_images_path = \"/content/Dataset/combined/images\"\n",
        "combined_labels_path = \"/content/Dataset/combined/labels\"\n",
        "\"\"\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(train_images_path, exist_ok=True)\n",
        "os.makedirs(train_labels_path, exist_ok=True)\n",
        "os.makedirs(test_images_path, exist_ok=True)\n",
        "os.makedirs(test_labels_path, exist_ok=True)\n",
        "\n",
        "# Combine all image and label files into one list\n",
        "all_images = os.listdir(combined_images_path)\n",
        "all_labels = os.listdir(combined_labels_path)"
      ],
      "metadata": {
        "id": "H1NyttpozJ7O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define your source and destination paths for train and test datasets\n",
        "train_images_path = \"/content/drive/MyDrive/Building_Defects/Dataset/train/images\"\n",
        "train_labels_path = \"/content/drive/MyDrive/Building_Defects/Dataset/train/labels\"\n",
        "test_images_path = \"/content/drive/MyDrive/Building_Defects/Dataset/test/images\"\n",
        "test_labels_path = \"/content/drive/MyDrive/Building_Defects/Dataset/test/labels\"\n"
      ],
      "metadata": {
        "id": "yc5I7KiOT_eU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset pipeline"
      ],
      "metadata": {
        "id": "sGOgEZCephlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(images, bboxes, labels, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, bboxes, labels))\n",
        "    dataset = dataset.shuffle(buffer_size=len(images))\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create train and test datasets\n",
        "batch_size = 16\n",
        "train_dataset = create_tf_dataset(train_images, train_bboxes, train_labels, batch_size)\n",
        "test_dataset = create_tf_dataset(test_images, test_bboxes, test_labels, batch_size)\n"
      ],
      "metadata": {
        "id": "A0YkGUo0nyAS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_loss(pred_bboxes, true_bboxes):\n",
        "    # Replace this with a custom YOLO loss function\n",
        "    return tf.reduce_mean(tf.square(pred_bboxes - true_bboxes))\n",
        "\n",
        "def classifier_loss(pred_labels, true_labels):\n",
        "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(true_labels, pred_labels)\n",
        "\n",
        "def combined_loss(bbox_loss, label_loss, alpha=1.0, beta=1.0):\n",
        "    return alpha * bbox_loss + beta * label_loss\n"
      ],
      "metadata": {
        "id": "4bcYUtRyplnF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the YOLO + Classifier Model"
      ],
      "metadata": {
        "id": "pBI421l5nzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from ultralytics import YOLO\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "# Load pretrained YOLO model\n",
        "yolo_model = YOLO('yolov5s.pt')  # Pretrained YOLOv8 weights\n",
        "\n",
        "def yolo_backbone(image):\n",
        "    \"\"\"\n",
        "    Use YOLO as a feature extractor and bounding box predictor.\n",
        "    \"\"\"\n",
        "    results = yolo_model(image)  # Forward pass through YOLO\n",
        "    bboxes = results[0].boxes.xyxy  # Extract bounding boxes\n",
        "    features = results[0].features  # Feature map from YOLO\n",
        "    return features, bboxes\n",
        "\n",
        "# Define the Classifier Head (e.g., VGG16 or Custom CNN)\n",
        "def build_classifier(input_shape=(7, 7, 512), num_classes=4):\n",
        "    \"\"\"\n",
        "    Build the classifier head for defect classification.\n",
        "    \"\"\"\n",
        "    base_model = tf.keras.applications.VGG16(\n",
        "        weights='imagenet', include_top=False, input_shape=input_shape\n",
        "    )\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Global pooling for feature reduction\n",
        "    x = Dense(256, activation='relu')(x)  # Fully connected layer\n",
        "    output = Dense(num_classes, activation='softmax')(x)  # Output layer\n",
        "    return Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Build the Unified Model\n",
        "class YOLOWithClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(YOLOWithClassifier, self).__init__()\n",
        "        self.yolo = yolo_model\n",
        "        self.classifier = build_classifier(num_classes=num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # YOLO for feature extraction and bounding box prediction\n",
        "        features, bboxes = yolo_backbone(inputs)\n",
        "\n",
        "        # ROI pooling: Use bounding boxes to extract features\n",
        "        rois = self.roi_pooling(features, bboxes)\n",
        "\n",
        "        # Classification head\n",
        "        predictions = self.classifier(rois)\n",
        "\n",
        "        return bboxes, predictions\n",
        "\n",
        "    def roi_pooling(self, features, bboxes):\n",
        "        \"\"\"\n",
        "        ROI pooling implementation for extracting bounding box features.\n",
        "        \"\"\"\n",
        "        pooled_features = []\n",
        "        for bbox in bboxes:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            roi = features[:, y1:y2, x1:x2, :]  # Crop the ROI\n",
        "            roi = tf.image.resize(roi, (7, 7))  # Resize to match classifier input\n",
        "            pooled_features.append(roi)\n",
        "        return tf.stack(pooled_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eksMOYb6nnp1",
        "outputId": "fb93f2c8-b160-4309-97d7-6a1b9a373d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "PRO TIP 💡 Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to 'yolov5su.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.7M/17.7M [00:00<00:00, 24.7MB/s]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_loss(pred_bboxes, true_bboxes):\n",
        "    \"\"\"\n",
        "    Compute YOLO bounding box loss (e.g., MSE for regression).\n",
        "    \"\"\"\n",
        "    return tf.reduce_mean(tf.square(pred_bboxes - true_bboxes))\n",
        "\n",
        "def classifier_loss(pred_labels, true_labels):\n",
        "    \"\"\"\n",
        "    Compute classification loss (e.g., Categorical Crossentropy).\n",
        "    \"\"\"\n",
        "    return tf.keras.losses.categorical_crossentropy(true_labels, pred_labels)\n",
        "\n",
        "def combined_loss(yolo_loss_value, classifier_loss_value, alpha=1.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Combine YOLO and classifier loss with weights alpha and beta.\n",
        "    \"\"\"\n",
        "    return alpha * yolo_loss_value + beta * classifier_loss_value\n"
      ],
      "metadata": {
        "id": "veThkh37n427"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "# Define your pathsPorosity Overweld\n",
        "categories = [ 'Porosity','Overweld','Underfilled', 'Undercut']\n",
        "label_map = {0: 'Porosity', 1: 'Overweld', 2: 'Underfilled', 3: 'Undercut'}"
      ],
      "metadata": {
        "id": "4evWeDRjZ0YT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the uploaded file\n",
        "zip_file_path = '/content/drive/MyDrive/Dataset-20250102T092628Z-001.zip'\n",
        "\n",
        "# Path to extract the contents\n",
        "extract_path = '/content'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsDOM5l14xpq",
        "outputId": "6a93da82-48d3-4531-8733-45e3c4154fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-dRqeD77QwM",
        "outputId": "da609d4d-8663-4815-a414-5ea778049bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/train\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the uploaded file\n",
        "zip_file_path = '/content/test.zip'\n",
        "\n",
        "# Path to extract the contents\n",
        "extract_path = '/content/test'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQHaS2K7TGE",
        "outputId": "e20fd127-bfaa-4e66-e9ec-37c2e709eda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/test\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Now you can download the zip files\n",
        "from google.colab import files\n",
        "files.download('/content/augmented_images_labels.zip')\n",
        "files.download('/content/augmented_labels.zip')\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "k8HAxGwfnVhp",
        "outputId": "52d2c243-d947-4f93-f92c-d3e6db7d254d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Now you can download the zip files\\nfrom google.colab import files\\nfiles.download('/content/augmented_images_labels.zip')\\nfiles.download('/content/augmented_labels.zip')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "\n",
        "# Paths\n",
        "input_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/images\"\n",
        "input_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/labels\"\n",
        "output_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/Agu/images\"\n",
        "output_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/Agu/labels\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(output_images_path, exist_ok=True)\n",
        "os.makedirs(output_labels_path, exist_ok=True)\n",
        "\n",
        "# Augmentation pipeline\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.Rotate(limit=15, p=0.4),\n",
        "        A.RandomBrightnessContrast(p=0.4),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]),\n",
        ")\n",
        "\n",
        "# Target number of images for each category\n",
        "target_images_per_category = {\n",
        "    0: 500,  # overweld\n",
        "    1: 500,  # porosity\n",
        "    2: 500,  # undercut\n",
        "    3: 500,  # underfilled\n",
        "}\n",
        "\n",
        "# List all image files in the images folder\n",
        "image_files = [f for f in os.listdir(input_images_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Function to get the category of an image from its label file\n",
        "def get_category_from_label(label_path):\n",
        "    with open(label_path, \"r\") as f:\n",
        "        class_id = int(f.readline().split()[0])  # First entry in label corresponds to the class\n",
        "    return class_id\n",
        "\n",
        "# Organize images by category\n",
        "images_by_category = {0: [], 1: [], 2: [], 3: []}\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(input_images_path, img_file)\n",
        "    label_path = os.path.join(input_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    # Get the category for the image\n",
        "    category = get_category_from_label(label_path)\n",
        "\n",
        "    # Add the image to the appropriate category list\n",
        "    images_by_category[category].append(img_file)\n",
        "\n",
        "# Process each category and augment images to meet the target\n",
        "for category, image_list in images_by_category.items():\n",
        "    category_name = categories[category]\n",
        "    current_image_count = len(image_list)\n",
        "    target_count = target_images_per_category[category]\n",
        "    augmentations_needed = target_count - current_image_count\n",
        "\n",
        "    print(f\"Processing category: {category_name} | Current: {current_image_count}, Target: {target_count}, Needed: {augmentations_needed}\")\n",
        "\n",
        "    if augmentations_needed > 0:\n",
        "        for img_file in image_list:\n",
        "            img_path = os.path.join(input_images_path, img_file)\n",
        "            image = cv2.imread(img_path)\n",
        "\n",
        "            label_path = os.path.join(input_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "            with open(label_path, \"r\") as f:\n",
        "                bboxes = []\n",
        "                class_labels = []\n",
        "                for line in f.readlines():\n",
        "                    parts = line.strip().split()\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center, y_center, width, height = map(float, parts[1:])\n",
        "                    bboxes.append([x_center, y_center, width, height])\n",
        "                    class_labels.append(class_id)\n",
        "\n",
        "            # Generate the required number of augmentations\n",
        "            for i in range(augmentations_needed):\n",
        "                augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "                augmented_image = augmented[\"image\"]\n",
        "                augmented_bboxes = augmented[\"bboxes\"]\n",
        "\n",
        "                # Save augmented image\n",
        "                aug_img_name = f\"aug_{category}_{i}_{img_file}\"\n",
        "                cv2.imwrite(os.path.join(output_images_path, aug_img_name), augmented_image)\n",
        "\n",
        "                # Save augmented labels\n",
        "                aug_label_name = f\"aug_{category}_{i}_{img_file.replace('.jpg', '.txt')}\"\n",
        "                with open(os.path.join(output_labels_path, aug_label_name), \"w\") as f:\n",
        "                    for bbox, class_id in zip(augmented_bboxes, class_labels):\n",
        "                        f.write(f\"{class_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n",
        "\n",
        "                augmentations_needed -= 1\n",
        "                if augmentations_needed == 0:\n",
        "                    break\n",
        "    else:\n",
        "        print(f\"Category '{category_name}' already meets or exceeds the target count. No augmentation needed.\")\n",
        "\n",
        "print(\"Augmentation completed!\")\n"
      ],
      "metadata": {
        "id": "lTrXwVxNnrT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from matplotlib import pyplot as plt\n",
        "categories = ['Porosity','Overweld', 'Underfilled', 'Undercut']\n",
        "label_map = {0: 'Porosity', 1: 'Overweld', 2: 'Underfilled', 3: 'Undercut'}\n",
        "# Paths\n",
        "input_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/images\"\n",
        "input_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/labels\"\n",
        "output_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/Testdata/images\"\n",
        "output_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/Testdata/labels\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(output_images_path, exist_ok=True)\n",
        "os.makedirs(output_labels_path, exist_ok=True)\n",
        "\n",
        "# Augmentation pipeline\n",
        "transform = A.Compose(\n",
        "    [\n",
        "\n",
        "        # **Additional Augmentations**\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Add Gaussian noise\n",
        "        A.MotionBlur(blur_limit=5, p=0.3),  # Apply motion blur  # Random affine transformations\n",
        "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),  # Contrast Limited Adaptive Histogram Equalization\n",
        "        A.ToGray(p=0.2),  # Convert image to grayscale with probability  # Simulate occlusions\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]),\n",
        ")\n",
        "\n",
        "# Target number of images for each category\n",
        "target_images_per_category = {\n",
        "    0: 100,  # overweld\n",
        "    1: 100,  # porosity\n",
        "    2: 100,  # undercut\n",
        "    3: 100,  # underfilled\n",
        "}\n",
        "\n",
        "# List all image files in the images folder\n",
        "image_files = [f for f in os.listdir(input_images_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Function to get the category of an image from its label file\n",
        "def get_category_from_label(label_path):\n",
        "    with open(label_path, \"r\") as f:\n",
        "        class_id = int(f.readline().split()[0])  # First entry in label corresponds to the class\n",
        "    return class_id\n",
        "\n",
        "# Organize images by category\n",
        "images_by_category = {0: [], 1: [], 2: [], 3: []}\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(input_images_path, img_file)\n",
        "    label_path = os.path.join(input_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    # Get the category for the image\n",
        "    category = get_category_from_label(label_path)\n",
        "\n",
        "    # Add the image to the appropriate category list\n",
        "    images_by_category[category].append(img_file)\n",
        "\n",
        "# Process each category and augment images to meet the target\n",
        "for category, image_list in images_by_category.items():\n",
        "    category_name = categories[category]\n",
        "    current_image_count = 0  # chaned code\n",
        "    target_count = target_images_per_category[category]\n",
        "    augmentations_needed = target_count - current_image_count\n",
        "\n",
        "    print(f\"Processing category: {category_name} | Current: {current_image_count}, Target: {target_count}, Needed: {augmentations_needed}\")\n",
        "\n",
        "    if augmentations_needed > 0:\n",
        "        for img_file in image_list:\n",
        "            img_path = os.path.join(input_images_path, img_file)\n",
        "            image = cv2.imread(img_path)\n",
        "\n",
        "            label_path = os.path.join(input_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "            with open(label_path, \"r\") as f:\n",
        "                bboxes = []\n",
        "                class_labels = []\n",
        "                for line in f.readlines():\n",
        "                    parts = line.strip().split()\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center, y_center, width, height = map(float, parts[1:])\n",
        "                    bboxes.append([x_center, y_center, width, height])\n",
        "                    class_labels.append(class_id)\n",
        "\n",
        "            # Generate the required number of augmentations\n",
        "            for i in range(augmentations_needed):\n",
        "                augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "                augmented_image = augmented[\"image\"]\n",
        "                augmented_bboxes = augmented[\"bboxes\"]\n",
        "\n",
        "                # Save augmented image\n",
        "                aug_img_name = f\"aug_{category}_{i}_{img_file}\"\n",
        "                cv2.imwrite(os.path.join(output_images_path, aug_img_name), augmented_image)\n",
        "\n",
        "                # Save augmented labels\n",
        "                aug_label_name = f\"aug_{category}_{i}_{img_file.replace('.jpg', '.txt')}\"\n",
        "                with open(os.path.join(output_labels_path, aug_label_name), \"w\") as f:\n",
        "                    for bbox, class_id in zip(augmented_bboxes, class_labels):\n",
        "                        f.write(f\"{class_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n",
        "\n",
        "                augmentations_needed -= 1\n",
        "                if augmentations_needed == 0:\n",
        "                    break\n",
        "    else:\n",
        "        print(f\"Category '{category_name}' already meets or exceeds the target count. No augmentation needed.\")\n",
        "\n",
        "print(\"Augmentation completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofSS01ugmYEw",
        "outputId": "0f204b47-2def-43cb-a567-37677073ad0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4b48f50a99dd>:22: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Add Gaussian noise\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing category: Porosity | Current: 0, Target: 100, Needed: 100\n",
            "Processing category: Overweld | Current: 0, Target: 100, Needed: 100\n",
            "Processing category: Underfilled | Current: 0, Target: 100, Needed: 100\n",
            "Processing category: Undercut | Current: 0, Target: 100, Needed: 100\n",
            "Augmentation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count images in each category\n",
        "def count_images_by_category(images_by_category, input_labels_path):\n",
        "    category_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "    for category, image_list in images_by_category.items():\n",
        "        category_counts[category] = len(image_list)\n",
        "    return category_counts\n",
        "\n",
        "# Print initial counts\n",
        "initial_counts = count_images_by_category(images_by_category, input_labels_path)\n",
        "print(\"Initial Counts Per Category:\")\n",
        "for category, count in initial_counts.items():\n",
        "    print(f\"Category {categories[category]} ({category}): {count} images\")\n",
        "\n",
        "# Augmentation process (no changes to previous code here)\n",
        "\n",
        "# Count images again after augmentation\n",
        "final_images_by_category = {0: [], 1: [], 2: [], 3: []}\n",
        "output_image_files = [f for f in os.listdir(output_images_path) if f.endswith('.jpg')]\n",
        "\n",
        "for img_file in output_image_files:\n",
        "    label_path = os.path.join(output_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "    category = get_category_from_label(label_path)\n",
        "    final_images_by_category[category].append(img_file)\n",
        "\n",
        "final_counts = count_images_by_category(final_images_by_category, output_labels_path)\n",
        "print(\"\\nFinal Counts Per Category After Augmentation:\")\n",
        "for category, count in final_counts.items():\n",
        "    print(f\"Category {categories[category]} ({category}): {count} images\")\n"
      ],
      "metadata": {
        "id": "nbBlDDf_q33k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06097c54-7c2d-42dc-ff5b-ecc34eb22f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Counts Per Category:\n",
            "Category Porosity (0): 32 images\n",
            "Category Overweld (1): 41 images\n",
            "Category Underfilled (2): 45 images\n",
            "Category Undercut (3): 45 images\n",
            "\n",
            "Final Counts Per Category After Augmentation:\n",
            "Category Porosity (0): 100 images\n",
            "Category Overweld (1): 100 images\n",
            "Category Underfilled (2): 100 images\n",
            "Category Undercut (3): 100 images\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count total images in all categories\n",
        "def count_total_images(category_counts):\n",
        "    return sum(category_counts.values())\n",
        "\n",
        "# Print initial counts\n",
        "initial_counts = count_images_by_category(images_by_category, input_labels_path)\n",
        "initial_total = count_total_images(initial_counts)\n",
        "\n",
        "print(\"Initial Counts Per Category:\")\n",
        "for category, count in initial_counts.items():\n",
        "    print(f\"Category {categories[category]} ({category}): {count} images\")\n",
        "print(f\"Total images before augmentation: {initial_total}\")\n",
        "\n",
        "# Augmentation process (no changes to previous code here)\n",
        "\n",
        "# Count images again after augmentation\n",
        "final_images_by_category = {0: [], 1: [], 2: [], 3: []}\n",
        "output_image_files = [f for f in os.listdir(output_images_path) if f.endswith('.jpg')]\n",
        "\n",
        "for img_file in output_image_files:\n",
        "    label_path = os.path.join(output_labels_path, img_file.replace(\".jpg\", \".txt\"))\n",
        "    category = get_category_from_label(label_path)\n",
        "    final_images_by_category[category].append(img_file)\n",
        "\n",
        "final_counts = count_images_by_category(final_images_by_category, output_labels_path)\n",
        "final_total = count_total_images(final_counts)\n",
        "\n",
        "print(\"\\nFinal Counts Per Category After Augmentation:\")\n",
        "for category, count in final_counts.items():\n",
        "    print(f\"Category {categories[category]} ({category}): {count} images\")\n",
        "print(f\"Total images after augmentation: {final_total}\")\n"
      ],
      "metadata": {
        "id": "3qQx6jVaq37d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e6ff05-4690-4ade-9bf6-81a7b4d85285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Counts Per Category:\n",
            "Category Porosity (0): 32 images\n",
            "Category Overweld (1): 41 images\n",
            "Category Underfilled (2): 45 images\n",
            "Category Undercut (3): 45 images\n",
            "Total images before augmentation: 163\n",
            "\n",
            "Final Counts Per Category After Augmentation:\n",
            "Category Porosity (0): 100 images\n",
            "Category Overweld (1): 100 images\n",
            "Category Underfilled (2): 100 images\n",
            "Category Undercut (3): 100 images\n",
            "Total images after augmentation: 400\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "original_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/images\"\n",
        "original_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/original/labels\"\n",
        "augmented_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/Agu/images\"\n",
        "augmented_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/Agu/labels\"\n",
        "\"\"\"\n",
        "original_images_path =  \"/content/drive/MyDrive/Building_Defects/Dataset/images\"\n",
        "original_labels_path = \"/content/drive/MyDrive/Building_Defects/Dataset/labels\"\n",
        "augmented_images_path = \"/content/drive/MyDrive/Building_Defects/Dataset/Aaugment_images\"\n",
        "augmented_labels_path = \"/content/drive/MyDrive/Building_Defects/Dataset/Aaugmented_labels\"\n",
        "\"\"\"\n",
        "combined_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/combined/images\"\n",
        "combined_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/combined/labels\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(combined_images_path, exist_ok=True)\n",
        "os.makedirs(combined_labels_path, exist_ok=True)\n",
        "\n",
        "# Function to copy images and labels to the combined folder with a label (original/augmented)\n",
        "def copy_images_and_labels(images_path, labels_path, destination_images_path, destination_labels_path, label_type):\n",
        "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]  # Adjust image extension if needed\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Copy image file to destination\n",
        "        src_image_path = os.path.join(images_path, image_file)\n",
        "        dst_image_path = os.path.join(destination_images_path, f\"{label_type}_{image_file}\")\n",
        "        shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "        # Copy corresponding label file to destination\n",
        "        label_file = image_file.replace(\".jpg\", \".txt\")  # Assuming .txt for label files\n",
        "        src_label_path = os.path.join(labels_path, label_file)\n",
        "        dst_label_path = os.path.join(destination_labels_path, f\"{label_type}_{label_file}\")\n",
        "        shutil.copy(src_label_path, dst_label_path)\n",
        "\n",
        "# Copy original images and labels with \"original\" label\n",
        "copy_images_and_labels(original_images_path, original_labels_path, combined_images_path, combined_labels_path, \"original\")\n",
        "\n",
        "# Copy augmented images and labels with \"augmented\" label\n",
        "copy_images_and_labels(augmented_images_path, augmented_labels_path, combined_images_path, combined_labels_path, \"augmented\")\n",
        "\n",
        "# Print the number of combined images\n",
        "combined_image_files = [f for f in os.listdir(combined_images_path) if f.endswith('.jpg')]\n",
        "print(f\"Total images in the combined folder: {len(combined_image_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPQHfROaXRgr",
        "outputId": "b8e172f1-f59f-4254-c8f6-83876d81972d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in the combined folder: 2000\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the combined total of original and augmented images\n",
        "def calculate_totals(images_by_category, augmented_labels_path):\n",
        "    # Initialize a dictionary to store totals for each category\n",
        "    combined_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "\n",
        "    # Count original images\n",
        "    for category, files in images_by_category.items():\n",
        "        combined_counts[category] += len(files)\n",
        "\n",
        "    # Count augmented images\n",
        "    augmented_label_files = [f for f in os.listdir(augmented_labels_path) if f.endswith('.txt')]\n",
        "    for label_file in augmented_label_files:\n",
        "        label_path = os.path.join(augmented_labels_path, label_file)\n",
        "        category = get_category_from_label(label_path)\n",
        "        combined_counts[category] += 1  # Add 1 for each augmented label in the category\n",
        "\n",
        "    return combined_counts\n",
        "\n",
        "# Calculate combined totals\n",
        "combined_counts = calculate_totals(images_by_category, output_labels_path)\n",
        "\n",
        "# Display combined totals and grand total\n",
        "print(\"Combined (Original + Augmented) Counts Per Category:\")\n",
        "grand_total = 0\n",
        "for category, count in combined_counts.items():\n",
        "    print(f\"Category {categories[category]} ({category}): {count} images\")\n",
        "    grand_total += count\n",
        "\n",
        "print(f\"Grand Total (All Categories): {grand_total} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgxKuVfVLLzL",
        "outputId": "6a4383c7-0066-4800-e8c3-cc7efc624e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined (Original + Augmented) Counts Per Category:\n",
            "Category Porosity (0): 500 images\n",
            "Category Overweld (1): 500 images\n",
            "Category Underfilled (2): 500 images\n",
            "Category Undercut (3): 500 images\n",
            "Grand Total (All Categories): 2000 images\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define paths\n",
        "combined_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/combined/images\"\n",
        "combined_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/combined/labels\"\n",
        "\n",
        "train_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/train/images\"\n",
        "train_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/train/labels\"\n",
        "test_images_path = \"/content/drive/MyDrive/Defects_welding/yolo/test/images\"\n",
        "test_labels_path = \"/content/drive/MyDrive/Defects_welding/yolo/test/labels\"\n",
        "\n",
        "# Ensure destination directories exist\n",
        "os.makedirs(train_images_path, exist_ok=True)\n",
        "os.makedirs(train_labels_path, exist_ok=True)\n",
        "os.makedirs(test_images_path, exist_ok=True)\n",
        "os.makedirs(test_labels_path, exist_ok=True)\n",
        "\n",
        "# Get list of image and label files\n",
        "image_files = sorted(os.listdir(combined_images_path))\n",
        "label_files = sorted(os.listdir(combined_labels_path))\n",
        "\n",
        "# Ensure each image has a corresponding label\n",
        "image_files = [img for img in image_files if os.path.splitext(img)[0] + \".txt\" in label_files]\n",
        "label_files = [lbl for lbl in label_files if os.path.splitext(lbl)[0] + \".jpg\" in image_files]\n",
        "\n",
        "# Perform an 80-20 split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    image_files, label_files, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to move files\n",
        "def move_files(file_list, source_folder, destination_folder):\n",
        "    for file_name in file_list:\n",
        "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(destination_folder, file_name))\n",
        "\n",
        "# Move training data\n",
        "move_files(train_images, combined_images_path, train_images_path)\n",
        "move_files(train_labels, combined_labels_path, train_labels_path)\n",
        "\n",
        "# Move testing data\n",
        "move_files(test_images, combined_images_path, test_images_path)\n",
        "move_files(test_labels, combined_labels_path, test_labels_path)\n",
        "\n",
        "print(\"Dataset successfully split and saved!\")"
      ],
      "metadata": {
        "id": "9SREvLNbz8Ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b2b371-cc44-4bf7-d250-780262553a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split and saved!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "td-gdD1WFT91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRTOn33pFUBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}