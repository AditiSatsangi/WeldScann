# -*- coding: utf-8 -*-
"""Detection - Yolo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqSC_QgRTGomvjQ-ZlVYKHwGp1yvRFBu

# Unified Model Architecture

Input Image
   → Pretrained YOLO (Feature Extraction + Bounding Box Predictions)
      → ROI Pooling Layer (Extract Features for Each Bounding Box)
         → Classifier Head (Fine-tuned VGG16 or Custom CNN for Defect Classification)
            → Combined Output (Bounding Boxes + Defect Labels)
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install ultralytics tensorflow keras

"""# Data Preprocessing

## Loding dataset
"""

import zipfile

# Path to the uploaded file
zip_file_path = '/content/train.zip'

# Path to extract the contents
extract_path = '/content/train'

# Unzipping the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"Files extracted to {extract_path}")

import zipfile

# Path to the uploaded file
zip_file_path = '/content/test.zip'

# Path to extract the contents
extract_path = '/content/test'

# Unzipping the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"Files extracted to {extract_path}")

import os
import tensorflow as tf

# Dataset loader with robustness
def load_dataset(image_dir, label_dir, target_size=(640, 640)):
    images = []
    bboxes = []
    labels = []
    missing_labels = []
    corrupt_labels = []

    for img_file in os.listdir(image_dir):
        try:
            # Load image
            img_path = os.path.join(image_dir, img_file)
            image = tf.io.read_file(img_path)
            image = tf.image.decode_jpeg(image, channels=3)
            image = tf.image.resize(image, target_size) / 255.0  # Normalize to [0, 1]

            # Load corresponding label
            label_file = os.path.join(label_dir, img_file.replace('.jpg', '.txt'))
            if not os.path.exists(label_file):
                missing_labels.append(img_file)
                continue  # Skip if label file is missing

            with open(label_file, 'r') as f:
                label_data = f.readlines()

            bbox = []
            class_ids = []
            for line in label_data:
                try:
                    # Parse YOLO label format
                    class_id, x_center, y_center, width, height = map(float, line.strip().split())
                    class_ids.append(int(class_id))

                    # Convert YOLO format to [x_min, y_min, x_max, y_max]
                    x_min = (x_center - width / 2) * target_size[0]
                    y_min = (y_center - height / 2) * target_size[1]
                    x_max = (x_center + width / 2) * target_size[0]
                    y_max = (y_center + height / 2) * target_size[1]
                    bbox.append([x_min, y_min, x_max, y_max])
                except ValueError:
                    corrupt_labels.append(label_file)
                    break  # Skip this image if label parsing fails

            if bbox:  # Add only if bounding boxes are valid
                images.append(image)
                bboxes.append(bbox)
                labels.append(class_ids)
        except Exception as e:
            print(f"Error processing file {img_file}: {e}")

    if missing_labels:
        print(f"Missing labels for {len(missing_labels)} images.")
    if corrupt_labels:
        print(f"Corrupt labels in {len(corrupt_labels)} files.")

    return images, bboxes, labels

# Paths to train/test folders
train_image_dir = '/content/drive/MyDrive/Building_Defects/Dataset/train/images'
train_label_dir = '/content/drive/MyDrive/Building_Defects/Dataset/train/labels'
test_image_dir = '/content/drive/MyDrive/Building_Defects/Dataset/test/images'
test_label_dir = '/content/drive/MyDrive/Building_Defects/Dataset/test/labels'

# Load train and test datasets
train_images, train_bboxes, train_labels = load_dataset(train_image_dir, train_label_dir)
test_images, test_bboxes, test_labels = load_dataset(test_image_dir, test_label_dir)

print(f"Loaded {len(train_images)} training images and {len(test_images)} test images.")

import shutil
import os
from sklearn.model_selection import train_test_split

# Paths for train and test datasets
train_images_path = "/content/drive/MyDrive/Building_Defects/train/images"
train_labels_path = "/content/drive/MyDrive/Building_Defects/train/labels"
test_images_path = "/content/drive/MyDrive/Building_Defect/test/images"
test_labels_path = "/content/drive/MyDrive/Building_Defects/test/labels"

combined_images_path = "/content/drive/MyDrive/Building_Defects/Dataset/combined/images"
combined_labels_path = "/content/drive/MyDrive/Building_Defects/Dataset/combined/labels"
"""
# Paths for train and test datasets
train_images_path = "/content/Dataset/train/images"
train_labels_path = "/content/Dataset/train/labels"
test_images_path = "/content/Dataset/test/images"
test_labels_path = "/content/Dataset/test/labels"

combined_images_path = "/content/Dataset/combined/images"
combined_labels_path = "/content/Dataset/combined/labels"
"""

# Create output directories if they don't exist
os.makedirs(train_images_path, exist_ok=True)
os.makedirs(train_labels_path, exist_ok=True)
os.makedirs(test_images_path, exist_ok=True)
os.makedirs(test_labels_path, exist_ok=True)

# Combine all image and label files into one list
all_images = os.listdir(combined_images_path)
all_labels = os.listdir(combined_labels_path)



"""## Preparing Dataset pipeline"""

def create_tf_dataset(images, bboxes, labels, batch_size=32):
    dataset = tf.data.Dataset.from_tensor_slices((images, bboxes, labels))
    dataset = dataset.shuffle(buffer_size=len(images))
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

# Create train and test datasets
batch_size = 16
train_dataset = create_tf_dataset(train_images, train_bboxes, train_labels, batch_size)
test_dataset = create_tf_dataset(test_images, test_bboxes, test_labels, batch_size)

def yolo_loss(pred_bboxes, true_bboxes):
    # Replace this with a custom YOLO loss function
    return tf.reduce_mean(tf.square(pred_bboxes - true_bboxes))

def classifier_loss(pred_labels, true_labels):
    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(true_labels, pred_labels)

def combined_loss(bbox_loss, label_loss, alpha=1.0, beta=1.0):
    return alpha * bbox_loss + beta * label_loss

path= '/content/drive/My Drive/Shared with me'

"""# Define the YOLO + Classifier Model"""

train_images_path = "/content/Datasets/train/images"
train_labels_path = "/content/Datasets/train/labels"
test_images_path = "/content/Datasets/test/images"
test_labels_path = "/content/Datasets/test/labels"

train_images_path

import os
import shutil
"""
# Define your source and destination paths for train and test datasets
train_images_path = "/content/drive/MyDrive/Building_Defects/Dataset/train/images"
train_labels_path = "/content/drive/MyDrive/Building_Defects/Dataset/train/labels"
test_images_path = "/content/drive/MyDrive/Building_Defects/Dataset/test/images"
test_labels_path = "/content/drive/MyDrive/Building_Defects/Dataset/test/labels"
"""

# Paths
# Define your paths
categories = [ 'Overweld', 'Porosity','Undercut', 'Underfilled']
label_map = {0: 'Overweld', 1: 'Porosity', 2: 'Undercut', 3: 'Underfilled'}

import zipfile

# Path to the uploaded file
zip_file_path = '/content/drive/MyDrive/Datasets-20250107T172536Z-001.zip'

# Path to extract the contents
extract_path = '/content'

# Unzipping the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"Files extracted to {extract_path}")

"""# Now you can download the zip files
from google.colab import files
files.download('/content/augmented_images_labels.zip')
files.download('/content/augmented_labels.zip')

# Model Training
"""

!pip install ultralytics

import os

# Define the directory where your .txt files are stored
annotation_dir = '/content/Datasets/train/labels'  # Change this to your folder path

# Loop through each file in the directory
for filename in os.listdir(annotation_dir):
    if filename.endswith('.txt'):
        file_path = os.path.join(annotation_dir, filename)

        # Open and read the file
        with open(file_path, 'r') as file:
            lines = file.readlines()

        # Create a new list to store modified lines
        modified_lines = []

        for line in lines:
            # Split the line into components: class_id, x_center, y_center, width, height
            parts = line.strip().split()
            # Modify the class_id to be '0' (or any other category)
            parts[0] = '0'  # Change this to the class you want

            # Recreate the modified line and append to the list
            modified_lines.append(" ".join(parts))

        # Overwrite the original file with the modified labels
        with open(file_path, 'w') as file:
            file.write("\n".join(modified_lines))

        print(f"Modified {filename}")

print("All files have been updated")

import os

# Define the directory where your .txt files are stored
annotation_dir = '/content/Datasets/test/labels'  # Change this to your folder path

# Loop through each file in the directory
for filename in os.listdir(annotation_dir):
    if filename.endswith('.txt'):
        file_path = os.path.join(annotation_dir, filename)

        # Open and read the file
        with open(file_path, 'r') as file:
            lines = file.readlines()

        # Create a new list to store modified lines
        modified_lines = []

        for line in lines:
            # Split the line into components: class_id, x_center, y_center, width, height
            parts = line.strip().split()
            # Modify the class_id to be '0' (or any other category)
            parts[0] = '0'  # Change this to the class you want

            # Recreate the modified line and append to the list
            modified_lines.append(" ".join(parts))

        # Overwrite the original file with the modified labels
        with open(file_path, 'w') as file:
            file.write("\n".join(modified_lines))

        print(f"Modified {filename}")

print("All files have been updated")

"""## Yolo8s"""

from ultralytics import YOLO
import torch

# Path to your dataset's YAML file
data_yaml_path = '/content/data (1).yaml'  # Ensure this YAML file is correctly configured

# Load YOLOv8 model with pre-trained weights
model = YOLO("yolov8s.pt")  # Using YOLOv8 weights

# Train the model
model.train(
    data=data_yaml_path,      # Path to dataset YAML
    epochs=20,               # Train for more epochs
    batch=16,                  # Experiment with batch sizes
    imgsz=640,               # Higher resolution images
    device='cuda' if torch.cuda.is_available() else 'cpu',  # Device
    lr0=0.001,                # Initial learning rate
    optimizer='AdamW',          # Optimizer choice (try 'AdamW' for experimentation)
    #augment=True,             # Enable augmentations
    freeze=[0, 1, 2],         # Freeze first few layers
)

"""### Evaluation"""

import locale

def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!sudo locale-gen en_US.UTF-8
!sudo update-locale LANG=en_US.UTF-8

# Path to your trained model (change this to the correct path)
model_path = '/content/runs/detect/train2/weights/best.pt'  # Replace with the correct path to the best model

# Evaluate the model
model = YOLO(model_path)

# Evaluate on the validation dataset.
results = model.val(data=data_yaml_path)

from IPython.display import Image

# Display an image from the evaluation results
eval_image_path = '/content/runs/detect/val/val_batch0_pred.jpg'  # Change to path of an eval image
Image(filename=eval_image_path)

# Print the evaluation
# print(results.pandas().xywh)  # This line is causing the error - Remove or comment it
print(results)  # Print the DetMetrics object to see its structure
print(f"mAP@0.5: {results.box.map50}")  # Access mAP@0.5 using results.box.map50
# or print(f"mAP@0.5: {results.box.maps[0.5]}")

import os
print(os.listdir('/content/runs'))

import zipfile

# Replace 'runs_detects' with the correct folder or file to zip
zip_filename = 'runs_detects.zip'
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    zipf.write('/content/runs')  # Make sure the file/folder exists

from google.colab import files
files.download('/content/runs_detects.zip')

import shutil

# Specify the folder you want to zip
folder_path = '/content/runs'

# Specify the name of the zip file
zip_filename = 'runs_detect_wwelding.zip'

# Zip the folder
# Adding the path to the zip file name for clarity
shutil.make_archive(os.path.join('/content', zip_filename), 'zip', folder_path)

"""## Yolo8n"""

from ultralytics import YOLO
import torch

# Path to your dataset's YAML file
data_yaml_path = '/content/data (1).yaml'  # Ensure this YAML file is correctly configured

# Load YOLOv8 model with pre-trained weights
model = YOLO("yolov8n.pt")  # Using YOLOv8 weights

# Train the model
model.train(
    data=data_yaml_path,      # Path to dataset YAML
    epochs=20,               # Train for more epochs
    batch=16,                  # Experiment with batch sizes
    imgsz=640,               # Higher resolution images
    device='cuda' if torch.cuda.is_available() else 'cpu',  # Device
    lr0=0.001,                # Initial learning rate
    optimizer='AdamW',          # Optimizer choice (try 'AdamW' for experimentation)
    #augment=True,             # Enable augmentations
    freeze=[0, 1, 2],         # Freeze first few layers
)

# Path to your trained model (change this to the correct path)
model_path = '/content/runs/detect/train/weights/best.pt'  # Replace with the correct path

# Evaluate the model
model = YOLO(model_path)

# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)
results = model.val(data=data_yaml_path)

from google.colab import files
files.download('/content/runs_detect_wwelding.zip.zip')

"""## Yolov8l"""

from ultralytics import YOLO
import torch

# Path to your dataset's YAML file
data_yaml_path = '/content/data (1).yaml'  # Ensure this YAML file is correctly configured

# Load YOLOv8 model with pre-trained weights
model2 = YOLO("yolov8l.pt")

# Train the model
model2.train(
    data=data_yaml_path,      # Path to dataset YAML
    epochs=20,               # Train for more epochs
    batch=16,                  # Experiment with batch sizes
    imgsz=640,               # Higher resolution images
    device='cuda' if torch.cuda.is_available() else 'cpu',  # Device
    lr0=0.001,                # Initial learning rate
    optimizer='AdamW',          # Optimizer choice (try 'AdamW' for experimentation)
    #augment=True,             # Enable augmentations
    freeze=[0, 1, 2],         # Freeze first few layers
)

# Path to your trained model (change this to the correct path)
model_path2 = '/content/runs/detect/train2/weights/best.pt'  # Replace with the correct path

# Evaluate the model
model2 = YOLO(model_path2)

# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)
results2 = model2.val(data=data_yaml_path)

"""## Yolov8m"""

from ultralytics import YOLO
import torch

# Path to your dataset's YAML file
data_yaml_path = '/content/data (1).yaml'  # Ensure this YAML file is correctly configured

# Load YOLOv8 model with pre-trained weights
model3 = YOLO("yolov8m.pt")

# Train the model
model3.train(
    data=data_yaml_path,      # Path to dataset YAML
    epochs=20,               # Train for more epochs
    batch=16,                  # Experiment with batch sizes
    imgsz=640,               # Higher resolution images
    device='cuda' if torch.cuda.is_available() else 'cpu',  # Device
    lr0=0.001,                # Initial learning rate
    optimizer='AdamW',          # Optimizer choice (try 'AdamW' for experimentation)
    #augment=True,             # Enable augmentations
    freeze=[0, 1, 2],         # Freeze first few layers
)

# Path to your trained model (change this to the correct path)
model_path3 = '/content/runs/detect/train3/weights/best.pt'  # Replace with the correct path

# Evaluate the model
model3 = YOLO(model_path3)

# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)
results3 = model3.val(data=data_yaml_path)

